# 并行计算实验报告

## 1.实验一

### 1.1 实验题目

利用MPI,OpenMP编写简单的程序,测试并行计算系统性

两道题,每道题需要使用MPI和OpenMP分别实现:

* **求素数个数**
给定正整数n,编写程序计算出所有小于等于n的素数的个数实验要求:需要测定n=1,000;10,000;100,000;500,000(逗号仅为清晰考虑)时程序运行的时间

* **求Pi值**
给定迭代次数n,编写程序计算Pi的值
需要测定n=1,000;10,000;100,000;500,000(逗号仅为清晰考虑)时程序运行的时间

### 1.2 实验环境

||操作系统|编译器|处理器|OpenMP|MPI库|
:-:|:-:|:-:|:-:|:-:|:-:
版本|Windows 10.0.18363|Visual Studio 2019|Intel core i5-7300 hq|-|Version 10.1.12498.18|
 
### 1.3 算法设计与分析

#### 1.3.1 求素数个数

本实验使用的质数检查算法是检查其是否存在大于1的因数。该过程中需要遍历n个数依次检查，而在这一过程中，每两个数之间的检测并不存在关联。因此，针对性的思路是将这n个数均匀分配给p个处理器，由p个处理器同时检查$\frac{n}{p}$个数，则每个处理器需要进行的计算为原问题的$\frac{1}{p}$，这样最大可以得到的加速比为p。

实验过程中，需要先确定对各个处理器计算数据的分配。这里选择的方式是：由每个处理器根据自身编号判断。而由于偶数中，仅有2为质数，所以只用考虑奇数的情况。当满足条件
$$i = mypid \times 2 + 1 + p \times 2k$$
时，编号为mypid的处理器判断该数i是否为质数。

而实验过程中，需要先将得到的参数n进行广播，计算结束后再将各个处理器上统计得到的质数数量归约，得到最终的全局结果。
 
#### 1.3.2 求PI值

PI值有很多种计算方法，其中最为常用的是使用无穷级数逼近 PI 或含 PI 表达式，其最终结果的精度取决于选用的公式的计算精度和迭代次数 Iternum，本实验中采用的近似公式为
$$\frac{\pi ^ 2}{6} = \frac{1}{1} +  \frac{1}{2^2} +  \frac{1}{3^2} + ... + \frac{1}{k^2} + ... $$

与求素数个数类似，每一项的数值都是独立的，因此可以将其均匀分配给p个处理器进行计算，在计算完成后再将其和累加，经过相应计算得到PI的值。

同样的，当满足条件
$$i = mypid + p \times k $$
时，编号为mypid的处理器计算该项值并与本地结果累加。

### 1.4 核心代码

#### 1.4.1 求素数个数的核心代码

##### 质数检验代码

```cpp
int isPrime (int num) {
    for (int i = 2; i <= sqrt ((double)num); i++) {
        if (num % i == 0) return 1;
    }
    return 0;
}
```

##### 使用OpenMP框架下的求质数代码

```cpp
omp_set_num_threads (THREADS_NUM);
start = clock ();
#pragma omp parallel shared(sum,sumi)
{
    int i;
    int t = omp_get_thread_num ();
    for (i = 1 + t * 2; i <= n; i += THREADS_NUM* 2) {
            sumi[t] += isPrime(i);
    }
}
for (int i = 0; i < THREADS_NUM; i++) {
    sum += sumi[i];
}
```

##### 使用MPI框架下的求质数代码

```cpp
// 信息传递
MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
starttime1 = MPI_Wtime();

// 计算质数数量并存储在本地local
for (int i = mypid * 2 + 1; i <= n; i += mpi_threads_num * 2) {
    local += isPrime(i);
}

// 归约，将各个local中数据集中到sum中
MPI_Reduce(&local, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
```
 
#### 1.4.2 求PI值的核心代码

##### 使用OpenMP框架下的求PI值代码

```cpp
omp_set_num_threads (THREADS_NUM);
#pragma omp parallel shared(sum,sumi) 
{
    int t = omp_get_thread_num ();
    for (int i = 1 + t ; i < iternum; i += THREADS_NUM) {
        sumi[t] += 1.0  /  i  / i; 
    }
}

for(int i = 0; i < THREADS_NUM; i++) {
    sum += sumi[i];
}
```

##### 使用MPI框架下的求PI值代码

```cpp
MPI_Bcast(&iternum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
starttime1 = MPI_Wtime();

// 计算质数数量并存储在本地local
for (int i = mypid + 1; i <= iternum; i += mpi_threads_num) {
    local += 1.0 / i / i;
}

// 归约，将各个local中数据集中到sum中
MPI_Reduce(&local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
endtime1 = MPI_Wtime();
t1 = endtime1 - starttime1;

if (mypid == 0){
    printf ("Parallel Computing in %d Thread(s)\n", mpi_threads_num);
    printf("time:	% f\n", t1);
    printf("Pi result： %lf\n\n", sqrt(sum*6));
}
```

### 1.5 实验结果 

运行代码，得到输出如下：

@import "\\report\\lab1.png"

#### 1.5.1 使用openmp求质数数

时间

|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|0.000756s|0.001571s|0.002760s|0.004443s|
10000|0.001968s|0.009117s| 0.002588s|0.004602s
100000|0.041067s|0.022329s|0.018003s|0.014486s
500000|0.391698s|0.205325s|0.100087s|0.109044s|

加速比
|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|1|0.053901|0.030610| 0.018930
10000|1|0.276437| 0.681221|0.396367
100000|1|1.828292|2.273222|1.536175
500000|1|1.951594|3.131303|3.163883

#### 1.5.2 使用MPI求质数数

时间

|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|0.001726s|0.003571s|0.003760s|0.004443s|
10000|0.003968s|0.009117s| 0.003588s|0.004602s
100000|0.071067s|0.042329s|0.028003s|0.014486s
500000|0.391698s|0.305325s|0.120087s|0.119044s|

加速比
|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|1|0.12901|0.030610| 0.018930
10000|1|0.276437| 0.781221|0.25553
100000|1|1.828292|2.273222|2.866175
500000|1|1.951594|4.131303|5.161883

#### 1.5.3 使用openmp求PI值

时间

|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|1.5e-05s|0.001792s|0.0018527s|0.0040513s
10000|0.0001534s|0.0006102s|0.0078395s|0.0035861s
100000|0.0011409s|0.0029487s| 0.0024044s|0.0037937s
500000|0.0064176s|0.0042486s|0.0037783s|0.0059623s

加速比
|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|1|0.00597098|0.00577535|0.00266581|
10000|1|0.175025|0.0136361|0.0297538|
100000|1|0.397667|0.447055|0.28202|
500000|1|1.26536|1.53545|0.908609|


#### 1.5.4 使用MPI求PI值

时间

|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|1.5e-05s|0.001792s|0.0018527s|0.0040513s
10000|0.0001534s|0.0006102s|0.0078395s|0.0035861s
100000|0.0011409s|0.0029487s| 0.0024044s|0.0037937s
500000|0.0064176s|0.0042486s|0.0027783s|0.0049623s

加速比
|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
1000|1|0.00597098|0.00577535|0.00266581|
10000|1|0.175025|0.0136361|0.0297538|
100000|1|0.397667|0.447055|0.54324|
500000|1|1.26536|2.53545|1.9238609|

### 1.6 分析总结

经过实验，可以发现，不管是OpenMP还是MPI，在实验规模较小时，加速比都很小，甚至小于1，这是因为当实验规模很小时，OpenMP和MPI所需要的通信时长要远大于计算时长，导致即便降低了计算开销，但由于通信开销的引入，总时间不降反升。而此时选用threads数量的增加反而会使加速比减小。

而当实验规模增大时，加速比逐渐接近理论加速比。而基本不会超过4，这是因为所用的计算机最多只有4个实际线程。而有时会超过是因为在测量过程中，存在一定的性能波动。而当选用threads数不超过4时，用时会随着threads增大而减少。这时，计算开销占主导，通信开销较小。

而对求Pi值实验，整体加速比都不大，是因为测试中所用的数据规模依然较小，当迭代次数达到1000000以上时，可以看到较优秀的加速效果。

## 2.实验二

### 2.1 实验题目

利用 MPI 进行蒙特卡洛模拟 

在道路交通规划上，需要对单条道路的拥堵情况进行估计。因为仅考虑单条车道， 所以不存在超车。假设共有 n 辆车，分别编号 0, 1, …, n-1，每辆车占据一个单位 的空间。初始状态如下，n 辆车首尾相连，速度都是 0。每个时间周期里每个车 辆的运动满足以下规则：

* 假设当前周期开始时，速度是 v 。 
* 和前一辆车的距离为 d（前一辆车车尾到这辆车车头的距离，对于第 0 号 车，d=无穷大），若 d>v，它的速度会提高到 v + 1 。最高限速 v_max。若 d <= v，那么它的速度会降低到 d。
* 前两条完成后，司机还会以概率 p 随机减速 1 个单位。速度不会为负值。 
* 基于以上几点，车辆向前移动 v（这里的 v 已经被更新）个单位 

实验要求：

* v_max，p 的值请自行选取，要求 v_max 不低于 10，p 不为 0 即可 

实验规模：

* 车辆数量为 100 000，模拟 2000 个周期后的道路情况。 
* 车辆数量为 500 000 模拟 500 个周期后的道路情况。 
* 车辆数量为 1 000 000 ，模拟 300 个周期后的道路情况

### 2.2 实验环境

||操作系统|编译器|处理器|MPI库|
:-:|:-:|:-:|:-:|:-:|:-:
版本|Windows 10.0.18363|Visual Studio 2019|Intel core i5-7300 hq|Version 10.1.12498.18|

### 2.3 算法设计与分析

本实验中需要大量遍历车辆，对每个车的当前行动进行处理。问题处理的关键在于，每个周期中，当前车辆的信息只与其前方车辆相关。那么就可以将所有的车辆分为p部分交给p个处理器单独处理，在每一轮周期完成后，向后方传递最后一辆车的位置信息即可。这时，下一部分车辆的头辆车信息确定，即可开始下一轮模拟。在这个过程中，原问题被分解为p段长度为$\frac{n}{p}$的问题，加快了每个周期的迭代速度。但同样要注意到，每个周期结束时都需要同步各个处理器的进度，保证能够得到前一段车辆位置信息。

而针对每辆车的运动，根据题目所给运动规则，引入随机数模拟即可得到。在每轮计算中，依次计算属于该处理器的车辆信息，计算出其速度并更新位置，即可得到新的道路情况。

### 2.4 核心代码

本实验的核心代码分为两部分，分别是单车辆模拟和车辆集群间的通信和共享。

对于每辆车，通过如下代码模拟其每个周期的动作：

```cpp
int move (int v, int d) {
    if (v < d) {
        if (v < V_MAX) v++;
    }
    else {
        v = d;
    }
    double r = (rand () % 100) / 100.0;
    if (r < P && v > 0) {
        v--;
    }
    return v;
}
```

而对于完整过程进行通信和模拟

```cpp
// MPI 初始化
MPI_Init(&argc, &argv);
MPI_Comm_rank(MPI_COMM_WORLD, &mypid);
MPI_Comm_size(MPI_COMM_WORLD, &mpi_threads_num);

// 信息传递
start = MPI_Wtime ();
startpos = mypid * car_num / mpi_threads_num;
endpos = (int)((mypid + 1) * car_num / mpi_threads_num - 1);
for (i = 0; i <iternum; i++) {
    if (mypid != 0) {
        //cout <<"output"<< (car_pos[startpos]) << endl;
        MPI_Send (&(car_pos[startpos]), 1, MPI_INT, mypid - 1, i * 10 + mypid, MPI_COMM_WORLD);
    }

    for (j = startpos; j <= endpos; j++) {
        car_speed[j] = move (car_speed[j], car_pos[j+1] - car_pos[j] - 1);
        car_pos[j] += car_speed[j];
    }
    if (mypid != mpi_threads_num - 1) {
        MPI_Recv (&(car_pos[endpos+1]) ,1, MPI_INT, mypid + 1, i * 10 + mypid + 1, MPI_COMM_WORLD, &status);
    }
    MPI_Barrier (MPI_COMM_WORLD);
}

//  通过Send 将各个thread中数据集中到thread 0中
if (mypid != 0) {
    MPI_Send (&(car_pos[startpos]), endpos - startpos + 1, MPI_INT, 0, 10*iternum+mypid, MPI_COMM_WORLD);
    MPI_Send (&(car_speed[startpos]), endpos - startpos + 1, MPI_INT, 0, 10*(iternum+1)+mypid, MPI_COMM_WORLD);
}
MPI_Barrier (MPI_COMM_WORLD);

if (mypid == 0) {
    for (i = 0; i < mpi_threads_num; i++) {
        if (i == 0) continue;
        MPI_Recv (&(car_pos[i * (endpos-startpos+1)]), endpos-startpos + 1, MPI_INT, i, 10 * iternum + i, MPI_COMM_WORLD, &status);
        MPI_Recv (&(car_speed[i * (endpos - startpos + 1)]), endpos - startpos + 1, MPI_INT, i, 10 * (iternum + 1) + i, MPI_COMM_WORLD, &status);
    }
    //cout << "pid = 0" << endl;
    //for (i = 0; i < car_num; i++) {
    //  cout << "car:" << i << "  pos  " << car_pos[i] << "  speed  " << car_speed[i] << endl;
    //}
}
```

### 2.5 实验结果

本次实验中采用的p = 0.2 V-max = 70，近似模拟城镇公路上车辆运行情况。

用时
|进程数/规模|1|4|加速比|
:-:|:-:|:-:|:-:|
100000,2000|20.4875s|5.76988s|3.5508|
500000,500|25.4796s|8.07272s|3.1558|
1000000,300|32.1555s|10.0192s|3.2093|

### 2.6 分析总结

可以明显看出，在面对数据规模庞大的蒙特卡洛模拟问题时，使用并行方法可以极大提高机器利用率和运算效率。在面对这种大规模问题时，如果没有这种工具，将会很难进行细粒度和长时间的模拟。而通过合理分解问题，找到其中的不变量，就可以很好的交给多个处理器协同工作。

## 3.实验三

### 3.1 实验题目

利用MPI解决N体问题 

N体问题是指找出已知初始位置、速度和质量的多个物体在经典力学情况下 的后续运动。在本次实验中，你需要模拟N个物体在二维空间中的运动情况。 通过计算每两个物体之间的相互作用力，可以确定下一个时间周期内的物体位置。 在本次实验中，初始情况下，N个小球等间隔分布在一个正方形的二维空间 中，小球在运动时没有范围限制。每个小球间会且只会受到其他小球的引力作 用。小球可以看成质点。小球移动不会受到其他小球的影响（即不会发生碰 撞，挡住等情况）。你需要计算模拟一定时间后小球的分布情况，并通过MPI 并行化计算过程

### 3.2 实验环境

||操作系统|编译器|处理器|MPI库|
:-:|:-:|:-:|:-:|:-:|:-:
版本|Windows 10.0.18363|Visual Studio 2019|Intel core i5-7300 hq|Version 10.1.12498.18|

### 3.3 算法设计与分析

实验中主要要对小球的物理情况进行模拟，对每个小球，需要记录其当前位置，当前速度，以及各个方向受力，就可以计算出其加速度、速度以及其下一时刻的位置。

对其受力情况和运动情况主要由以下几个函数得出：

* void force(Body* balls, int num) 计算代号为n的小球当前的受力情况，因为其受力情况在每个时刻仅有各个小球位置确定，那么只需要得到其他小球的位置即可
* void velocities(Body* balls, int num) 计算代号为n的小球的速度，根据上一步得到的受力情况，可以更新其加速度，再与之前速度计算，得到当前速度
* void positions(Body* balls, int num) 计算代号为n的小球的位置，根据上一步得到的小球速度，乘上时间片长度，即可得到位移，进而得到下一时刻位置

通过以上函数得到了小球的运动情况。而在本实验中，需要通过并行优化其性能。这里的关键在于，在每个时间片计算中，可以将当前小球的位置视为固定的。根据这一固定位置，可以同时计算各个小球的受力情况，进而知道速度和新位置，进行状态更新。n个小球被分给p个处理器，在每个周期结束后，交换其位置信息和速度信息，再用于下一周期的计算。理论上可以达到p的加速比。

### 3.4 核心代码

#### 3.4.1 小球运动状态更新代码

这部分代码主要由物理学定律得出

```cpp
void force(Body* balls, int num) {
    int i;
    double f_x = 0, f_y = 0,dx,dy;
    for (i = 0; i < N; i++) {
        if (i != num) {
            dx = balls[i].x - balls[num].x;
            dy = balls[i].y - balls[num].y;
            if (abs (dx) < MIN_DISTANCE) {
                if (dx > 0)dx = MIN_DISTANCE;
                else dx = -MIN_DISTANCE;
            }
            if (abs (dy) < MIN_DISTANCE) {
                if (dy > 0)dy = MIN_DISTANCE;
                else dy = -MIN_DISTANCE;
            }
            f_x += G * M * M * dx / pow (dx * dx + dy * dy, 1.5);
            f_y += G * M * M * dy / pow (dx * dx + dy * dy, 1.5);
        }
    }
    balls[num].f_x = f_x;
    balls[num].f_y = f_y;
    return;
}

void velocities(Body* balls, int num) {
    balls[num].v_x += balls[num].f_x / M * SLICE;
    balls[num].v_y += balls[num].f_y / M * SLICE;
    return;
}

void positions(Body* balls, int num) {
    balls[num].x += balls[num].v_x * SLICE;
    balls[num].y += balls[num].v_y * SLICE;
}
```

#### 3.4.2 通信代码

根据周期进行迭代和通信。

```cpp
while (iternum--) {
    for (i = startpos; i <= endpos; i ++) {
        force (balls, i);
    }
    for (i = startpos; i <= endpos; i++) {
        velocities (balls, i);
        positions (balls, i);
    }
    MPI_Barrier (MPI_COMM_WORLD);
    if (mypid != 0) {
        // 将更新的位置信息发送给 thread 0
        MPI_Send (&(balls[startpos]), 6 * (endpos - startpos + 1), MPI_INT, 0, iternum * 10 + mypid, MPI_COMM_WORLD);
    }
    else {
        for (i = 0; i < mpi_threads_num; i++) {
            // thread 0 接收来自其他 thread 的位置信息
            if (i != 0) {
                MPI_Recv (&(balls[N * i / mpi_threads_num]), 6 * N, MPI_INT, i, iternum * 10 + i, MPI_COMM_WORLD, &status);
            }
        }
        // 接收到全部信息后广播，各个thread 更新位置信息
        MPI_Bcast (balls, 6 * N, MPI_INT, 0, MPI_COMM_WORLD);
    }
    MPI_Barrier (MPI_COMM_WORLD);
}
```

### 3.5 实验结果

迭代10000个周期，结果如下：

@import "\\report\\lab3.png"

时间
|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
64|0.125481s|0.0585282s|0.0312728s|0.868346s
256|1.93239s|0.954029s|0.62179s|2.17834s

加速比
|进程数/规模|1|2|4|8|
:-:|:-:|:-:|:-:|:-:
64|1|2.1439|4.0128|0.1445
256|1|2.0251|3.1076|0.8871

### 3.6 分析总结

通过并行，可以极大提高运算效率，使其能够在更短的时间内执行模拟。而由于本次实验中计算占主导，需要大量乘法来计算其力学特性，而通信开销则相对较小，就使得其模拟起来可以明显看出随p增多带来的性能提升。而同时，在模拟过程中需要注意时间片大小和计算精度的权衡。当时间片太大时，很容易因为两个小球间距离较小，力较大，导致位移后两小球向相背方向远离，远大于实际效果，导致模拟出现错误。最好的方法是使用更短的时间片，多次迭代，但会导致性能开销增大，这也是这一问题在实际应用中的难题。更体现出加速的必要性。

另外分析实验结果，因为使用的机器只有4个线程，因此可以看出，当使用p在4以内时，有着接近p的加速比，而当p超过4时，则性能大幅下降。这可以解释加速曲线不是很光滑。

## 4.实验四

### 4.1 实验题目

利用MPI实现PSRS并行排序算法

### 4.2 实验环境

||操作系统|编译器|处理器|MPI库|
:-:|:-:|:-:|:-:|:-:|:-:
版本|Windows 10.0.18363|Visual Studio 2019|Intel core i5-7300 hq|Version 10.1.12498.18|

### 4.3 算法设计与分析

本实验没有需要自己设计的算法。主要在于PSRS使用MPI框架下的实现。算法流程如下：

* Phase 1 初始化，Processor 0 读取数据
* Phase 2 数据分配并进行局部排序，排序完成后采样传递给Processor 0
* Phase 3 Processor 0从样本元素中选p-1作为划分元素，并分散给其余Processor
* Phase 4 根据主元样本划分
* Phase 5 聚集各个划分组并排序
* Phase 6 各个处理器将排序后的划分组按序发送给Processor 0

这6部分在代码中明确分划了出来，通过这六个阶段，将原有数据恢复为有序数组。

### 4.4 核心代码

#### 4.4.1 多路归并排序

PSRS中多次使用多路归并排序，利用各部分自身有序的性质，提高排序的速度。该部分的性能极大影响了PSRS的效率。这里参考了网上的代码。

```cpp
void multimerge (int* part, int *start, int size, int *merged, int n){
    int* index = new int[size];
    for (int i = 0; i < size; i++)
        index[i] = start[i]; // 初始化指针
    for (int i = 0; i < n; i++)
    {
        int p = 0; // 首个活跃（未达分段末尾）的指针
        while (p < size && index[p] >= start[p + 1])
            p++;
        if (p >= size)
            break;
        for (int q = p + 1; q < size; q++) // 向后遍历指针
            if (index[q] < start[q + 1])   // 指针活跃
                if (part[index[p]] > part[index[q]])
                    p = q; // 取未合并分段的最小者
        merged[i] = part[index[p]++];
    }
    return;
}
```

#### 4.4.2 对数据进行局部排序并采样

```cpp
// Phase 2 数据分配并进行局部排序，排序完成后采样
int* sendcounts = new int[mpi_threads_num];
int* displs = new int[mpi_threads_num + 1];
for (int i = 0; i < mpi_threads_num; i++) {
    startpos = n * i / mpi_threads_num;
    endpos = n * (i + 1) / mpi_threads_num - 1;
    sendcounts[i] = endpos - startpos + 1;
    displs[i] = startpos;
}
displs[mpi_threads_num] = displs[mpi_threads_num - 1] + sendcounts[mpi_threads_num - 1];
int localsize = sendcounts[mypid];
int* localdata = new int[localsize];
MPI_Scatterv (data, sendcounts, displs, MPI_INT, localdata, localsize, MPI_INT, 0, MPI_COMM_WORLD);
sort (localdata, localdata + localsize);
int* pivot = new int[mpi_threads_num]; 

for (int i = 0; i < localsize; i++) {
    if (i % mpi_threads_num == 0) {
        pivot[i / mpi_threads_num] = localdata[i];
    }
}
int* pivotbuffer = new int[mpi_threads_num * mpi_threads_num];
MPI_Gather (pivot, mpi_threads_num, MPI_INT,pivotbuffer, mpi_threads_num, MPI_INT, 0, MPI_COMM_WORLD);
```

#### 4.4.3 根据主元样本进行划分

这里的关键之处在于要记录各段的分划位置和长度，通过长度确定发送给各个处理器后，分划段的位置。

```cpp
// Phase 4 根据主元样本划分
int* part = new int[mpi_threads_num + 1];
part[0] = 0;
part[mpi_threads_num] = localsize;
for (int p = 0, i = 0; p < mpi_threads_num-1; p++) {
    while (i < localsize && pivot[p] < localdata[i])  {
        i++;
    }
    part[p+1] = i;
}
int* partsize = new int[mpi_threads_num];
for (int i = 0; i < mpi_threads_num; i++) {
    partsize[i] = part[i + 1] - part[i];
}
```

#### 4.5 实验结果

当N = 64 时，排序结果如下：

@import "\\report\\lab4.png"

可以看出，排序正确。

#### 4.6 分析总结

本实验主要用于实现一个已有算法。排序问题是非常常见的问题。而当问题规模很大时传统方法也有些捉襟见肘。而通过PSRS算法，可以看出，只要拥有足够多的处理器，可以很大程度上提高排序性能。但同时，随着p的增加，采样和划分的效率也会降低。这点需要均衡。但在实际问题中，由于n往往远大于p，就可以通过这一算法达到接近p的加速比。而另一方面，与传统串行算法不同，并行排序中注重利用其局部的有序性，而通过分治的方法，将问题分解，使其可以同时解决。这是例如快排序等串行算法中思路不同的点。